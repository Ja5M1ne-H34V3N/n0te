# 复杂度 和大O记法（ing）

所有者: H34V3N

衡量程序，算法的好坏 我们用他们消耗的计算资源（时间 空间） 来计算

算法要考虑时间和空间复杂度 ：

时间复杂度用于描述算法执行时间随输入规模增长而增长的趋势，它并不表示算法具体的执行时间，而是一种渐进的分析方式，反映了算法执行时间与输入规模之间的关系。

此时 我们发现 赋值语句同时包含计算（表达式 ） 和存储两个功能

在介绍计算方式之前，我们先介绍几个定义：

## 基本操作：

在一串代码中 我们总能找到一些和问题规模无关的语句 比如赋值语句和比较语句，以及一些基本的加减乘除，其执行时间相对是固定的，我们称其为基本操作。

## 问题规模n：

问题规模是影响算法执行时间的主要因素，在算法里 通常指输入数据的大小和数量

比如在寻求n个数的和时，要确定的数字的个数n就是问题规模

我们算法分析的目的就是找出问题规模为什么会影响算法的执行时间

## 基本操作数量函数：

基本操作数量函数（记作T（n））是以问题规模为自变量，基本操作数量为因变量的函数，其能够反映问题规模与运行时间的关系

比如：

```python
n = int（input()）
for i in range(n):
	i = n
```

中随着问题规模n的增大其T（n）表示为 1（n = input()单次赋值） + n（for循环n次中i - n赋值）

T(n) =1 + n

（这里为了简化计算，不考虑for循环是否进行的判断语句，其实其消耗时间相对固定，也通常被忽略不记）

## 大O记法：

在基本操作数量函数的确定过程中 如果函数较为复杂，我们会发现自变量n在表达式中有不同的数量级（比如多次嵌套循环导致的n的多次方），在这种变量的引领下，当n逐渐增大时，数量级较大的项会主导这个函数，数量级较小的项对函数值得影响越来越小，所以我们保留这个最大的数量级，同时，这个最大数量级的系数（比如 2n中的 2）也对增长的影响相对较小 我们也会省略。这既是大O记法

大O像一条渐近线，随着n增大，基本操作函数的图像会越来愈接近这个O

我们看一看最基本的循环语句：

for( i = 0;i < n;i++)

…….

}

这之中的基本操作函数是2（n + 1），其时间复杂度就是O（n）（后面说）

斐波那契数列

pyhton：

```python
def fibonacci(n):
    # 初始化前两项
    fib = [0] * (n)
    fib[0] = fib[1] = 1
    # 从第 3 项开始迭代计算
    for i in range(2, n):
        fib[i] = fib[i - 1] + fib[i - 2]
    return fib[n]
```

观察这个代码 设定数值的占用消耗小到可以忽视

其T（n）为：2（初始化前两项） +3 *（ n - 2）（循环体的操作,循环n - 2次） + 1（ fib = [0] * (n + 1)） + 1（return）= 3n - 2

时间复杂度O（ n ） 是线性的

空间复杂度 长度n的列表存储数据 1个i存储循环判断中的变量 共n + 1

接着 我们换一种方法 不用数组，减少空间消耗

```python
def fib(n):
    if n == 1 or n == 2:
        return 1
    a = b = 1
    for i in range(3, n + 1):
        c = a + b
        a = b
        b = c
    return b
```

利用开头的两个数固定 通过第三个数存储前面两个数的和得到第三个数，一直往前走

然而此时的时间复杂度还是线性的

但是其空间复杂度减少到了3 ： 就用了三个变量存储数据

（第一个就用数组存储[1,1,2,3,5,7,…….]）

看下一个函数

`sum(A,B,int n){`

`for(i = 0;i < n;i++){`

`for(j = 0;j < n;j++){`

`s[i,j] = A[i,j] + B[i,j]`

`}}`

`}`

时间复杂度分析：

for(i = 0;i < n;i++) : n + 1

for(j = 0;j < n;j++): n(n + 1)

s[i,j] = A[i,j] + B[i,j] n * n

空间复杂度分析：

种类 words

ABS三个数组：

n * n

nij三个整形：

共3n * n + 3

总结：一次循环

fpr(i = 1;i < n;i++){

c = a + b}

时间复杂度：n(次循环) + 1（函数体语句）

空间复杂度: 有四个整形abci 为4

两次嵌套循环：

for( i = 1;i < n;i++){

for(k = 0;k < n;k++){

c = a + b}

}

时间复杂度:两个for：n(n + 1)

for里的语句：n^ 2次

总和：2 n^ 2 + n

接下来 我们以[二分搜索](https://www.notion.so/1d605a2041d28043bb9dc9f91ef0055f?pvs=21)为例，来研究分情况的时间复杂度

# 分析算法在不同情况下的消耗：

在一列数里有 1 2 3 4 5 6 7 8按顺序排好的数，我们随即给出一个其中的数，找到这个数在这个数组中的位置。

我们通过遍历得到目标：

for i in range(9):

if i == target:

break

**最好的情况：**

我们要找的是第一个数，循环了一次就找到了，执行的次数确定是1+ 1（return），此时的时间复杂度是O(1)

**最坏的情况：**

我们要找的总是数组中的最后一个数，如果你要找的是n（比如给出的数组里面是8），那么，就固定要循环n次，这样，n次循环，加上判断每次循环一次判断语句，还要加上return时间复杂度就是 n * 1  + 1= n + 1，舍去1，时间复杂度就是 O(n)

**平均情况：**

就是列出所有情况的算式，求平均值

（1 + 2 + 3 +……+n）/ n = n / 2

平均复杂度也是O(n)，线性的

通过[二分查找](https://www.notion.so/1d605a2041d28043bb9dc9f91ef0055f?pvs=21)得到目标：

```python
nums = [1, 2, 3, 4, 5, 6, 7]
target = 3
low = 0
high = len(nums) - 1

while True:
    mid = (low + high) // 2
    if nums[mid] > target:
        high = mid - 1
    elif nums[mid] < target:
        low = mid + 1
    else:
        break
print(mid + "为所求")
```

时间复杂度分析：

这个过程就像把绳子一次次对折 ，一次舍弃一半的数字 直到找到目标值 这时 ，目标值恰好是当前范围最大最小值的中间，也就是说 我们要给总数除以多少次2 才能得到目标值

也就形成一个二叉树

![image.png](复杂度%20和大O记法（ing）/image.png)

**最好的情况：**

我们的目标正好在整个数组的正中间，while循环了一次就找到了目标，这样，时间复杂度就是O(n)

**最坏的情况：**

我们二分了最多次数，在分无可分时终于找到目标，此时假设分了k次，则有：

$2^k = n$ - 1化简得：$k = log2(n-1)$

所以 循环了log2(n - 1)次，时间复杂度就是O（log2（n））

**平均情况:**

看上图得二叉树，我们发现，每一个节点都是一个值，那么我们把他们每个节点搜索到的复杂度相加，再除以整数就好

假设一共能分成h（h =log2(n - 1 ））层树，我们发现第h层有$2^(h - 1)$个元素，想查找到这层元素 需要 h次比较

所以 其平均复杂度能写成

$(2 ^ 0 * 1 + 2^1 * 2 + ...+2 ^ h * h - 1) / (2^h - 1)$  约等于log2(n)

所以 时间复杂度时O(log2(n))

### 复杂时间的定义：

恒定时间（有界时间）：算法的时间复杂度不受输入大小的影响

线性时间：时间复杂度随着问题规模的增大而线性增大，通常呈一次函数。（如简单的循环遍历）

平方时间：通常涉及将一个线性算法应用n次（比如循环套循环）

对数时间：通过反复将输入减半来缩小搜索范围，直到找到目标值（比如二分）

Nlog2N：将一个对数算法使用n次

指数时间 阶乘时间：算法成本太高，平时遇不到，遇到也是绕道走，所以不赘述。

与n log2 n 成正比的复杂度问题：

线性函数： 随着问题规模增大，我们发现复杂度会直接成i下那行增加 输入加一倍 复杂度就加一倍

对数函数 对于log2n这个函数来说 如果n翻倍 函数值就只会加1

与n方成正比的复杂度问题：

如果问题规模增加一倍 复杂度直接成为原来的四倍

与2^n有关的复杂度问题

问题规模变成原来的二倍，结果将……

所以我们在编程时要尽量避免复杂度太高的算法，优化自己的思路

# 由增长率带来的消耗变化：

但是 有时候“看起来”更简单的算法在一定范围内不一定最好

比如下面两张图：

表示了两个算法的问题规模和复杂度的对比图

可以发现前期 带三次方的其实占用资源更少

![image.png](复杂度%20和大O记法（ing）/image%201.png)

到了后期……

![image.png](复杂度%20和大O记法（ing）/image%202.png)

尽管前期较为复杂的算法消耗偏低 但是这也只是庞大数据中的“个例”，所以大O的算法类似于极限的定义，结果通常是增速最大的那一部分主导的

![image.png](复杂度%20和大O记法（ing）/image%203.png)

不同的量级的大小比较大概是这样的……